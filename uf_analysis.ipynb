{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import gower\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from numpy import arange\n",
    "from numpy import hstack\n",
    "from scipy import stats\n",
    "from numpy import meshgrid\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import KDTree\n",
    "from IPython.display import display\n",
    "from pandas.errors import EmptyDataError\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from scipy.spatial.distance import _validate_vector\n",
    "from scipy.stats import median_absolute_deviation\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ufce\n",
    "from ufce import UFCE\n",
    "from goodness import *\n",
    "from cfmethods import *\n",
    "from evaluations import *\n",
    "from data_processing import *\n",
    "from generate_text_explanations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "ufc = UFCE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this file contain the experiment code for user-feedback analysis with different levels of user-constraints, specificaly customised for Bank Loan dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_percentage_for_uf_analysis(df, f2c,fcat, seed):\n",
    "    # Compute the median absolute deviation of each feature\n",
    "    mad = df.std()\n",
    "    # Add 5% of the MAD to each feature and store the updated values in a dictionary\n",
    "    updated_values = {}\n",
    "    list_update_values = []\n",
    "    for feature in f2c:\n",
    "        if feature == 'Education':\n",
    "            updated_values[feature] = 1\n",
    "        else:\n",
    "            if feature == 'CCAvg':\n",
    "                updated_values[feature] = round(mad[feature]*seed, 2)\n",
    "            else:\n",
    "                updated_values[feature] = round(mad[feature] * seed)\n",
    "    for feature in fcat:\n",
    "        updated_values[feature] = 1\n",
    "    list_update_values.append(updated_values)\n",
    "    return list_update_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_testinstance(test, cf, uf):\n",
    "    temp = test.copy()\n",
    "    for feature in uf.keys():\n",
    "        temp[feature] = test[feature].values[0] + uf[feature]\n",
    "    for f in cf:\n",
    "        if temp[f].values < cf[f].values or cf[f].values < test[f].values:\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Read data\n",
    "#path = r'/home/m.suffian/Downloads/UFCE-4GPU/data/'  # use this path on ubuntu. make sure you have correct path to UFCE folder.\n",
    "path = r'C:\\Users\\laboratorio\\Documents\\GitHub\\UFCE\\data' # use this path format on windows system, verify your drive path to UFCE\n",
    "pathbank = r'C:\\Users\\laboratorio\\Documents\\GitHub\\UFCE\\data\\bank.csv'\n",
    "datasetdf = pd.read_csv(pathbank)\n",
    "datasetdf = datasetdf.sample(frac=1)\n",
    "# print(datasetdf.mad(), datasetdf.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, lr_mean, lr_std, Xtest, Xtrain, X, Y, df = classify_dataset_getModel(datasetdf[:4000], data_name='bank') # this method returns trained ML model's, cleaned dataframe, and etc. #mlp, mlp_mean, mlp_std, \n",
    "models = {lr: [lr_mean, lr_std]}  #, mlp: [mlp_mean, mlp_std]}\n",
    "print(\"cross-val mean score of lr\", lr_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Bank Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readpath = r'C:\\Users\\laboratorio\\Documents\\GitHub\\UFCE\\folds\\bank\\totest\\testfold_1_pred_0.csv'\n",
    "writepath = r'C:\\Users\\laboratorio\\Documents\\GitHub\\UFCE\\folds\\bank\\totest\\\\'\n",
    "features, catf, numf, uf, f2changee, outcome_label, desired_outcome, nbr_features, protectf, data_lab0, data_lab1 = get_bank_user_constraints(df) # this will return user-constraints specific to data set.\n",
    "# del data_lab1['Unnamed: 0']\n",
    "# del data_lab1['age']\n",
    "# del data_lab1['Experience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take top mutual information sharing pairs of features\n",
    "MI_FP = ufc.get_top_MI_features(X, features)\n",
    "print(f'\\t Top-5 Mutually-informed feature paris:{MI_FP[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2change = ['Income', 'CCAvg', 'Mortgage', 'Education']\n",
    "f2cat = ['CDAccount', 'Online']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different levels of values (specific percentage of MAD represents to a diferent user value)\n",
    "uf1 = compute_percentage_for_uf_analysis(df, f2change, f2cat, 0.20) # 20% of mad\n",
    "uf2 = compute_percentage_for_uf_analysis(df, f2change, f2cat, 0.40) # 40% of mad\n",
    "uf3 = compute_percentage_for_uf_analysis(df, f2change, f2cat, 0.60) # 60% of mad\n",
    "uf4 = compute_percentage_for_uf_analysis(df, f2change, f2cat, 0.80) # 80% of mad\n",
    "uf5 = compute_percentage_for_uf_analysis(df, f2change, f2cat, 1.0) # # 100% of mad\n",
    "ufs = [uf3] #uf1, uf2, uf3, uf4, \n",
    "# print(ufs)\n",
    "models = ['lr'] # the experiment is run for 'lr' (Logistic Regression) as AR method doesn't work on 'mlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnames = ['ufce1','ufce2', 'ufce3', 'dice', 'ar']\n",
    "percent_cfs_all = pd.DataFrame()\n",
    "time_cfs_all = pd.DataFrame()\n",
    "for u, uf in enumerate(ufs):\n",
    "    print(f' user feedback{u}:', uf[0])\n",
    "    cfmethods = ['DiCE', 'AR', 'UFCE1', 'UFCE2', 'UFCE3']\n",
    "    methodtimes = dict()\n",
    "    cfcounts = dict()\n",
    "    no_cf = 1\n",
    "    k = 1\n",
    "    bb = lr\n",
    "    desired_outcome = desired_outcome\n",
    "    protectf = protectf\n",
    "    meandf_catproximity = pd.DataFrame(columns=mnames)\n",
    "    stddf_catproximity = pd.DataFrame(columns=mnames)\n",
    "    meandf_contproximity = pd.DataFrame(columns=mnames)\n",
    "    stddf_contproximity = pd.DataFrame(columns=mnames)\n",
    "    meandf_sparsity = pd.DataFrame(columns=mnames)\n",
    "    stddf_sparsity = pd.DataFrame(columns=mnames)\n",
    "    meandf_actionability = pd.DataFrame(columns=mnames)\n",
    "    stddf_actionability = pd.DataFrame(columns=mnames)\n",
    "    meandf_plausibility = pd.DataFrame(columns=mnames)\n",
    "    stddf_plausibility = pd.DataFrame(columns=mnames)\n",
    "    meandf_feasibility = pd.DataFrame(columns=mnames)\n",
    "    stddf_feasibility = pd.DataFrame(columns=mnames)\n",
    "    meandf_diversity = pd.DataFrame(columns=mnames)\n",
    "    stddf_diversity = pd.DataFrame(columns=mnames)\n",
    "    meandf_feasibility = pd.DataFrame(columns=mnames)\n",
    "    stddf_feasibility = pd.DataFrame(columns=mnames)\n",
    "    cols = ['ufce1mean', 'ufce2mean', 'ufce3mean','dicemean', 'armean',  'ufce1std', 'ufce2std',\n",
    "            'ufce3std', 'dicestd', 'arstd']\n",
    "    jproxidf = pd.DataFrame(columns=cols)\n",
    "    catproxidf = pd.DataFrame(columns=cols)\n",
    "    contproxidf = pd.DataFrame(columns=cols)\n",
    "    spardf = pd.DataFrame(columns=cols)\n",
    "    actdf = pd.DataFrame(columns=cols)\n",
    "    plausdf = pd.DataFrame(columns=cols)\n",
    "    feasidf = pd.DataFrame(columns=cols)\n",
    "    # for i, file in enumerate(testfolds[:]):\n",
    "    #     print(f'\\t\\t\\t Test Fold:{i} ')\n",
    "    #     try:\n",
    "    #         testset = pd.read_csv(file)\n",
    "    #     except pd.io.common.EmptyDataError:\n",
    "    #         print('File is empty or not found')\n",
    "    #     else:\n",
    "    testset = pd.read_csv(readpath)\n",
    "    totest1 = pd.DataFrame()\n",
    "    totest2 = pd.DataFrame()\n",
    "    totest3 = pd.DataFrame()\n",
    "    for i, method in enumerate(cfmethods):\n",
    "        print(f'\\t\\t\\t\\t Method: {method}  --------------')\n",
    "        k = 3\n",
    "        if method == 'DiCE':\n",
    "            dicecfs_v, dicecfs, methodtimes[i], flag1 = dice_cfexp(df, testset[:k], numf, f2changee, 1, bb, uf[0])\n",
    "            dicecfs_v = dicecfs_v.drop('Personal Loan', axis=1)\n",
    "            if flag1 != 0:\n",
    "                dicecfs = dicecfs.drop('Personal Loan', axis=1)\n",
    "                # count = 0\n",
    "                # for x in range(len(testset[:k])):\n",
    "                #     flag = modify_testinstance(testset[x:x+1], dicecfs[x:x+1], uf[0])\n",
    "                #     if flag:\n",
    "                #         count = count + 1\n",
    "                cfcounts[i] = len(dicecfs)/len(testset[:k]) * 100\n",
    "            else:\n",
    "                cfcounts[i] = 0\n",
    "            #print(f'\\t\\t\\t\\t Counterfactuals \\t:{dicecfs.values}')\n",
    "            print(f\"---DiCE generated {len(dicecfs)} with UF and {len(dicecfs_v)} without UF-----------\")\n",
    "        elif method == 'AR':\n",
    "            arcfs_v, arcfs, methodtimes[i], flag1 = ar_cfexp(X, numf, bb, testset[:k], uf[0])\n",
    "            if flag1 != 0:\n",
    "                # count = 0\n",
    "                # for x in range(len(testset[:k])):\n",
    "                #     flag = modify_testinstance(testset[x:x + 1], arcfs[x:x + 1], uf[0])\n",
    "                #     if flag:\n",
    "                #         count = count + 1\n",
    "                cfcounts[i] = len(arcfs) / len(testset[:k]) * 100\n",
    "            else:\n",
    "                cfcounts[i] = 0\n",
    "            #print(f'\\t\\t\\t\\t Counterfactual \\t:{arcfs.values}')\n",
    "        elif method == 'UFCE1':\n",
    "            step = {'Age':1, 'Experience':1, 'Income':1, 'CCAvg':.1, 'Family':1, 'Education':1, 'Mortgage':1, 'Securities Account':1,'CD Account':1,'Online':1, 'CreditCard':1}\n",
    "            onecfs, methodtimes[i], foundidx1, interval1, testout1 = sfexp(X, data_lab1, testset[:k], uf[0], step, f2changee, numf, catf, bb, desired_outcome, k)\n",
    "            \n",
    "            if len(onecfs) != 0:\n",
    "                totest1 = testset.loc[foundidx1]\n",
    "                totest1 = totest1.reset_index(drop=True)\n",
    "                onecfs = onecfs.reset_index(drop=True)\n",
    "                cfcounts[i] = len(onecfs)/len(testset[:k]) * 100\n",
    "            else:\n",
    "                cfcounts[i] = 0\n",
    "            # print(\"---------------1F idx---:\", foundidx1)\n",
    "            # for id in foundidx1:\n",
    "            #     print(f'\\t\\t\\t\\t{id} Test instance \\t:{testset[id:id+1].values}')\n",
    "            #     print(f'\\t\\t\\t\\t UF with MC \\t:{interval1[id]}')\n",
    "            #     print(f'\\t\\t Counterfactual \\t:{onecfs[id:id+1].values}')\n",
    "        elif method == 'UFCE2':\n",
    "            twocfs, methodtimes[i], foundidx2, interval2, testout2 = dfexp(X, data_lab1, testset[:k], uf[0], MI_FP[:5], numf, catf, f2changee, protectf, bb, desired_outcome, k)\n",
    "            \n",
    "            if len(twocfs) != 0:\n",
    "                totest2 = testset.loc[foundidx2]\n",
    "                totest2 = totest2.reset_index(drop=True)\n",
    "                twocfs = twocfs.reset_index(drop=True)\n",
    "                cfcounts[i] = len(twocfs)/len(testset[:k]) * 100\n",
    "            else:\n",
    "                cfcounts[i] = 0\n",
    "            # print(\"---------------2F idx---:\", foundidx2)\n",
    "            \n",
    "            # for id in foundidx2:\n",
    "            #     print(f'\\t\\t\\t\\t{id} Test instance \\t:{testset[id:id + 1].values}')\n",
    "            #     print(f'\\t\\t\\t\\t UF with MC \\t:{interval2[id]}')\n",
    "            #     print(f'\\t\\t\\t\\t Counterfactual \\t:{twocfs[id:id + 1].values}')\n",
    "        else:\n",
    "            threecfs, methodtimes[i], foundidx3, interval3, testout3 = tfexp(X, data_lab1, testset[:k], uf[0], MI_FP[:5], numf, catf, f2changee, protectf, bb, desired_outcome, k)#features\n",
    "            \n",
    "            if len(threecfs) != 0:\n",
    "                totest3 = testset.loc[foundidx3]\n",
    "                totest3 = totest3.reset_index(drop=True)\n",
    "                threecfs = threecfs.reset_index(drop=True)\n",
    "                cfcounts[i] = len(threecfs)/len(testset[:k]) * 100\n",
    "            else:\n",
    "                cfcounts[i] = 0\n",
    "            # print(\"---------------3F idx---:\", foundidx3)\n",
    "            # for id in foundidx3:\n",
    "            #     print(f'\\t\\t{id} Test instance \\t:{testset[id:id + 1].values}')\n",
    "            #     print(f'\\t\\t UF with MC \\t:{interval3[id]}')\n",
    "            #     print(f'\\t\\t Counterfactual \\t:{threecfs[id:id + 1].values}')\n",
    "\n",
    "    # calling all 7 evaluation metrics (properties)\n",
    "    # # categorical proximity\n",
    "    mmeans, mstds = [], []\n",
    "    # mmeans, mstds = Catproximity(onecfs, totest1, twocfs, totest2, threecfs, totest3, dicecfs, arcfs, testset[:k], catf)\n",
    "    # df = pd.DataFrame(data=[mmeans], columns=mnames)\n",
    "    # meandf_catproximity = pd.concat([meandf_catproximity, df], ignore_index=True, axis=0)\n",
    "    # df = pd.DataFrame(data=[mstds], columns=mnames)\n",
    "    # stddf_catproximity = pd.concat([stddf_catproximity, df], ignore_index=True, axis=0)\n",
    "    # mmeans.extend(mstds)\n",
    "    # df = pd.DataFrame(data=[mmeans], columns=cols)\n",
    "    # catproxidf = pd.concat([catproxidf, df], ignore_index=True, axis=0)\n",
    "    # # continuous proximity\n",
    "    # mmeans, mstds = [], []\n",
    "    # mmeans, mstds = Contproximity(onecfs, totest1, twocfs, totest2, threecfs, totest3, dicecfs, arcfs, testset[:k], numf)#Xtest\n",
    "    # df = pd.DataFrame(data=[mmeans], columns=mnames)\n",
    "    # meandf_contproximity = pd.concat([meandf_contproximity, df], ignore_index=True, axis=0)\n",
    "    # df = pd.DataFrame(data=[mstds], columns=mnames)\n",
    "    # stddf_contproximity = pd.concat([stddf_contproximity, df], ignore_index=True, axis=0)\n",
    "    # mmeans.extend(mstds)\n",
    "    # df = pd.DataFrame(data=[mmeans], columns=cols)\n",
    "    # contproxidf = pd.concat([contproxidf, df], ignore_index=True, axis=0)\n",
    "    # # sparsity\n",
    "    # mmeans, mstds = [], []\n",
    "    # mmeans, mstds = Sparsity(onecfs, totest1, twocfs, totest2, threecfs, totest3, dicecfs, arcfs, testset[:k], numf)#Xtest\n",
    "    # df = pd.DataFrame(data=[mmeans], columns=mnames)\n",
    "    # meandf_sparsity = pd.concat([meandf_sparsity, df], ignore_index=True, axis=0)\n",
    "    # df = pd.DataFrame(data=[mstds], columns=mnames)\n",
    "    # stddf_sparsity = pd.concat([stddf_sparsity, df], ignore_index=True, axis=0)\n",
    "    # mmeans.extend(mstds)\n",
    "    # df = pd.DataFrame(data=[mmeans], columns=cols)\n",
    "    # spardf = pd.concat([spardf, df], ignore_index=True, axis=0)\n",
    "    # # actionability\n",
    "    # mmeans, mstds = [], []\n",
    "    # mmeans, mstds = Actionability(onecfs, totest1, twocfs, testout2, threecfs, testout3, dicecfs, arcfs, testset[:k], features, f2change)#Xtest\n",
    "    # df = pd.DataFrame(data=[mmeans], columns=mnames)\n",
    "    # meandf_actionability = pd.concat([meandf_actionability, df], ignore_index=True, axis=0)\n",
    "    # df = pd.DataFrame(data=[mstds], columns=mnames)\n",
    "    # stddf_actionability = pd.concat([stddf_actionability, df], ignore_index=True, axis=0)\n",
    "    # mmeans.extend(mstds)\n",
    "    # df = pd.DataFrame(data=[mmeans], columns=cols)\n",
    "    # actdf = pd.concat([actdf, df], ignore_index=True, axis=0)\n",
    "    # mmeans, mstds = [], []\n",
    "    # mmeans, mstds = Plausibility(onecfs, testout1, twocfs, testout2, threecfs, testout3, dicecfs, arcfs, testset[:k], Xtrain)#Xtest\n",
    "    # df = pd.DataFrame(data=[mmeans], columns=mnames)\n",
    "    # meandf_plausibility = pd.concat([meandf_plausibility, df], ignore_index=True, axis=0)\n",
    "    # df = pd.DataFrame(data=[mstds], columns=mnames)\n",
    "    # stddf_plausibility = pd.concat([stddf_plausibility, df], ignore_index=True, axis=0)\n",
    "    # mmeans.extend(mstds)\n",
    "    # df = pd.DataFrame(data=[mmeans], columns=cols)\n",
    "    # plausdf = pd.concat([plausdf, df], ignore_index=True, axis=0)\n",
    "    # mmeans, mstds = [], []\n",
    "    mmeans, mstds = Feasibility(onecfs, totest1, twocfs, totest2, threecfs, totest3, dicecfs, dicecfs_v, arcfs, arcfs_v, testset[:k], Xtrain, features, f2changee, bb, desired_outcome, outcome_label, uf[0])#Xtest\n",
    "    df = pd.DataFrame(data=[mmeans], columns=mnames)\n",
    "    meandf_feasibility = pd.concat([meandf_feasibility, df], ignore_index=True, axis=0)\n",
    "    df = pd.DataFrame(data=[mstds], columns=mnames)\n",
    "    stddf_feasibility = pd.concat([stddf_feasibility, df], ignore_index=True, axis=0)\n",
    "    mmeans.extend(mstds)\n",
    "    df = pd.DataFrame(data=[mmeans], columns=cols)\n",
    "    feasidf = pd.concat([feasidf, df], ignore_index=True, axis=0)\n",
    "\n",
    "    # here storing the time and percentage of counterfactuals for each cfmethod.\n",
    "    temptime = pd.DataFrame([methodtimes])\n",
    "    time_cfs_all = pd.concat([time_cfs_all, temptime], ignore_index=True)\n",
    "    tempcount = pd.DataFrame([cfcounts])\n",
    "    percent_cfs_all = pd.concat([percent_cfs_all, tempcount], ignore_index=True)\n",
    "    # print(\"Time of all methods for this uf\")\n",
    "    # print(display(time_cfs_all))\n",
    "    # print(\"Percentage of all methods for this uf\")\n",
    "    # print(display(percent_cfs_all))\n",
    "    # print(f'\\t\\t\\t\\t-----fold_mean_values of all evaluation metrics----')\n",
    "\n",
    "    # print(f'Mean and St.dev of Joint-Proximity:', jproxidf.to_latex(float_format=\"{:0.2f}\".format))\n",
    "    # print(f'Mean and St.dev of Cat-Proximity:', catproxidf.to_latex(float_format=\"{:0.2f}\".format))\n",
    "    # print(f'Mean and St.dev of Cont-Proximity:', contproxidf.to_latex(float_format=\"{:0.2f}\".format))\n",
    "    # print(f'Mean and St.dev of Sparsity:', spardf.to_latex(float_format=\"{:0.2f}\".format))\n",
    "    # print(f'Mean and St.dev of Actionability:', actdf.to_latex(float_format=\"{:0.2f}\".format))\n",
    "    print(f'Mean and St.dev of Plausibility:', plausdf.to_latex(float_format=\"{:0.2f}\".format))\n",
    "    print(f'Mean and St.dev of Feasibility:', feasidf.to_latex(float_format=\"{:0.2f}\".format))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
